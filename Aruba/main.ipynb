{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Load dataset\n",
      "STEP 2: prepare dataset\n",
      "STEP 3: segment dataset in sequence of activity\n",
      "[          date  time sensor value  activity    log\n",
      "0   2010-11-04     0   M003    ON  Sleeping  begin\n",
      "1   2010-11-04     0   M003   OFF  Sleeping  begin\n",
      "2   2010-11-04     2   M003    ON  Sleeping  begin\n",
      "3   2010-11-04     2   M003   OFF  Sleeping  begin\n",
      "4   2010-11-04     3   M003    ON  Sleeping  begin\n",
      "5   2010-11-04     3   M003   OFF  Sleeping  begin\n",
      "6   2010-11-04     3   M003    ON  Sleeping  begin\n",
      "7   2010-11-04     3   M003   OFF  Sleeping  begin\n",
      "8   2010-11-04     4   M003    ON  Sleeping  begin\n",
      "9   2010-11-04     4   M002    ON  Sleeping  begin\n",
      "10  2010-11-04     4   M002   OFF  Sleeping  begin\n",
      "11  2010-11-04     4   M003   OFF  Sleeping  begin\n",
      "12  2010-11-04     4   M003    ON  Sleeping  begin\n",
      "13  2010-11-04     4   M002    ON  Sleeping  begin\n",
      "14  2010-11-04     4   M002   OFF  Sleeping  begin\n",
      "15  2010-11-04     4   M003   OFF  Sleeping  begin\n",
      "16  2010-11-04     5   M003    ON  Sleeping  begin\n",
      "17  2010-11-04     5   M002    ON  Sleeping  begin\n",
      "18  2010-11-04     5   M007    ON  Sleeping  begin\n",
      "19  2010-11-04     5   M002   OFF  Sleeping  begin\n",
      "20  2010-11-04     5   M007   OFF  Sleeping  begin\n",
      "21  2010-11-04     5   M003   OFF  Sleeping  begin\n",
      "22  2010-11-04     5   M003    ON  Sleeping  begin\n",
      "23  2010-11-04     5   M007    ON  Sleeping  begin\n",
      "24  2010-11-04     5   M003   OFF  Sleeping    end,           date  time sensor value activity  log\n",
      "25  2010-11-04     5   M003    ON    Other  end\n",
      "26  2010-11-04     5   M005    ON    Other  end\n",
      "27  2010-11-04     5   M003   OFF    Other  end,           date  time sensor value       activity    log\n",
      "28  2010-11-04     5   M004    ON  Bed_to_Toilet  begin\n",
      "29  2010-11-04     5   M005   OFF  Bed_to_Toilet  begin\n",
      "30  2010-11-04     5   M007   OFF  Bed_to_Toilet  begin\n",
      "31  2010-11-04     5   M004   OFF  Bed_to_Toilet  begin\n",
      "32  2010-11-04     5   M004    ON  Bed_to_Toilet  begin\n",
      "33  2010-11-04     5   M004   OFF  Bed_to_Toilet  begin\n",
      "34  2010-11-04     5   M007    ON  Bed_to_Toilet  begin\n",
      "35  2010-11-04     5   M004    ON  Bed_to_Toilet  begin\n",
      "36  2010-11-04     5   M007   OFF  Bed_to_Toilet  begin\n",
      "37  2010-11-04     5   M007    ON  Bed_to_Toilet  begin\n",
      "38  2010-11-04     5   M005    ON  Bed_to_Toilet  begin\n",
      "39  2010-11-04     5   M004   OFF  Bed_to_Toilet    end,           date  time sensor value activity  log\n",
      "40  2010-11-04     5   M005   OFF    Other  end\n",
      "41  2010-11-04     5   M007   OFF    Other  end\n",
      "42  2010-11-04     5   M007    ON    Other  end\n",
      "43  2010-11-04     5   M007   OFF    Other  end,            date  time sensor value  activity    log\n",
      "44   2010-11-04     5   M003    ON  Sleeping  begin\n",
      "45   2010-11-04     5   M003   OFF  Sleeping  begin\n",
      "46   2010-11-04     5   M002    ON  Sleeping  begin\n",
      "47   2010-11-04     5   M003    ON  Sleeping  begin\n",
      "48   2010-11-04     5   M002   OFF  Sleeping  begin\n",
      "49   2010-11-04     5   M003   OFF  Sleeping  begin\n",
      "50   2010-11-04     5   M003    ON  Sleeping  begin\n",
      "51   2010-11-04     5   M003   OFF  Sleeping  begin\n",
      "52   2010-11-04     5   M003    ON  Sleeping  begin\n",
      "53   2010-11-04     5   M003   OFF  Sleeping  begin\n",
      "54   2010-11-04     6   M003    ON  Sleeping  begin\n",
      "55   2010-11-04     6   M003   OFF  Sleeping  begin\n",
      "56   2010-11-04     6   M003    ON  Sleeping  begin\n",
      "57   2010-11-04     6   M003   OFF  Sleeping  begin\n",
      "58   2010-11-04     6   M003    ON  Sleeping  begin\n",
      "59   2010-11-04     6   M003   OFF  Sleeping  begin\n",
      "60   2010-11-04     6   M003    ON  Sleeping  begin\n",
      "61   2010-11-04     6   M003   OFF  Sleeping  begin\n",
      "62   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "63   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "64   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "65   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "66   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "67   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "68   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "69   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "70   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "71   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "72   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "73   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "74   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "75   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "76   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "77   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "78   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "79   2010-11-04     7   M002    ON  Sleeping  begin\n",
      "80   2010-11-04     7   M002   OFF  Sleeping  begin\n",
      "81   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "82   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "83   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "84   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "85   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "86   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "87   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "88   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "89   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "90   2010-11-04     7   M003    ON  Sleeping  begin\n",
      "91   2010-11-04     7   M003   OFF  Sleeping  begin\n",
      "92   2010-11-04     8   M003    ON  Sleeping  begin\n",
      "93   2010-11-04     8   M003   OFF  Sleeping  begin\n",
      "94   2010-11-04     8   M003    ON  Sleeping  begin\n",
      "95   2010-11-04     8   M007    ON  Sleeping  begin\n",
      "96   2010-11-04     8   M003   OFF  Sleeping  begin\n",
      "97   2010-11-04     8   M007   OFF  Sleeping  begin\n",
      "98   2010-11-04     8   M003    ON  Sleeping  begin\n",
      "99   2010-11-04     8   M007    ON  Sleeping  begin\n",
      "100  2010-11-04     8   M003   OFF  Sleeping  begin\n",
      "101  2010-11-04     8   M003    ON  Sleeping  begin\n",
      "102  2010-11-04     8   M003   OFF  Sleeping    end]\n",
      "STEP 4: transform sequences of activity in sentences\n",
      "['M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M002ON M002OFF M003OFF M003ON M002ON M002OFF M003OFF M003ON M002ON M007ON M002OFF M007OFF M003OFF M003ON M007ON M003OFF', 'M003ON M005ON M003OFF', 'M004ON M005OFF M007OFF M004OFF M004ON M004OFF M007ON M004ON M007OFF M007ON M005ON M004OFF', 'M005OFF M007OFF M007ON M007OFF', 'M003ON M003OFF M002ON M003ON M002OFF M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M002ON M002OFF M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M003OFF M003ON M007ON M003OFF M007OFF M003ON M007ON M003OFF M003ON M003OFF']\n",
      "STEP 5: sentences indexization\n",
      "STEP 6: split indexed sentences in sliding windows\n",
      "STEP 7: pad sliding windows\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def load_raw_dataset(input_file_s):\n",
    "    df_s = pd.read_csv(input_file_s, sep=\"\\t\", header=None,\n",
    "                       names=[\"date\", \"time\", \"sensor\", \"value\", \"activity\", \"log\"])\n",
    "    return df_s\n",
    "\n",
    "\n",
    "def clean_and_prepare(df_s):\n",
    "    df_s.log = df_s.log.fillna(method='ffill')\n",
    "    df_s['activity'] = df_s['activity'].fillna(df_s['log'])\n",
    "    df_s['activity'] = df_s['activity'].replace(\"end\", \"Other\")\n",
    "    df_s['activity'] = df_s['activity'].fillna(\"Other\")\n",
    "    df_s['activity'] = df_s['activity'].replace(\"begin\", None)\n",
    "    df_s['activity'] = df_s['activity'].fillna(method='ffill')\n",
    "    return df_s\n",
    "\n",
    "\n",
    "def save_activity_dict(df_s, input_file_s):\n",
    "    filename = \"aruba_activity_list_step1.pickle\"\n",
    "    activities = df_s.activity.unique()\n",
    "    # activities.sort()\n",
    "    dictActivities = {}\n",
    "    for i_s, activity in enumerate(activities):\n",
    "        dictActivities[activity] = i_s\n",
    "    pickle_out = open(filename, \"wb\")\n",
    "    pickle.dump(dictActivities, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "\n",
    "def generate_sentence(df2):\n",
    "    sentence = \"\"\n",
    "    sensors = df2.sensor.values\n",
    "    values = df2.value.values\n",
    "    for i_s in range(len(sensors)):\n",
    "        val = values[i_s]\n",
    "        if i_s == len(sensors) - 1:\n",
    "            sentence += \"{}{}\".format(sensors[i_s], val)\n",
    "        else:\n",
    "            sentence += \"{}{} \".format(sensors[i_s], val)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def segment_activities(df_s):\n",
    "    activitiesSeq = []\n",
    "    ponentialIndex = df_s.activity.ne(df_s.activity.shift())\n",
    "    ii = np.where(ponentialIndex == True)[0]\n",
    "    for i_s, end in enumerate(ii):\n",
    "        if i_s > 0:\n",
    "            df_stmp = df_s[ii[i_s - 1]:end]\n",
    "            activitiesSeq.append(df_stmp)\n",
    "    return activitiesSeq\n",
    "\n",
    "\n",
    "def sliding_window(sequence, win_size_s, step_s=1):\n",
    "    try:\n",
    "        iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    # if not (isinstance(type(win_size_s), type(0)) and (isinstance(type(step_s), type(0)))):\n",
    "    #     raise Exception(\"**ERROR** type(win_size_s) and type(step_s) must be int.\")\n",
    "    # if step_s > win_size_s:\n",
    "    #     raise Exception(\"**ERROR** step_s must not be larger than win_size_s.\")\n",
    "    numOfChunks = int(((len(sequence) - win_size_s) / step_s) + 1)\n",
    "\n",
    "    if win_size_s > len(sequence):\n",
    "        yield sequence[0:len(sequence)]\n",
    "    else:\n",
    "        for i_s in range(0, numOfChunks * step_s, step_s):\n",
    "            yield sequence[i_s:i_s + win_size_s]\n",
    "\n",
    "\n",
    "def sequences_to_sentences(activity_sequences_s):\n",
    "    sentences_s = []\n",
    "    label_sentences_s = []\n",
    "    for i_s in range(len(activity_sequences_s)):\n",
    "        sentence = generate_sentence(activity_sequences_s[i_s])\n",
    "        sentences_s.append(sentence)\n",
    "        label_sentences_s.append(activity_sequences_s[i_s].activity.values[0])\n",
    "    return sentences_s, label_sentences_s\n",
    "\n",
    "input_file = r\"no_D123\"\n",
    "win_size = 100\n",
    "step = 1\n",
    "\n",
    "print(\"STEP 1: Load dataset\")\n",
    "df = pd.read_csv(\"data\",\n",
    "                 sep='\\t', header=None, names=[\"date\", \"time\", \"sensor\",\n",
    "                                               \"value\", \"activity\", \"log\"])\n",
    "\n",
    "print(\"STEP 2: prepare dataset\")\n",
    "df = clean_and_prepare(df)\n",
    "save_activity_dict(df, input_file)\n",
    "\n",
    "print(\"STEP 3: segment dataset in sequence of activity\")\n",
    "activity_sequences = segment_activities(df)\n",
    "print(activity_sequences[0:5])\n",
    "df_txt = df.iloc[:, :-2]\n",
    "\n",
    "print(\"STEP 4: transform sequences of activity in sentences\")\n",
    "sentences, label_sentences = sequences_to_sentences(activity_sequences)\n",
    "print(sentences[0:5])\n",
    "\n",
    "print(\"STEP 5: sentences indexization\")\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "indexed_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(\"STEP 6: split indexed sentences in sliding windows\")\n",
    "X_windowed = []\n",
    "Y_windowed = []\n",
    "X_windowed_sen = []\n",
    "Y_windowed_sen = []\n",
    "for i, s in enumerate(indexed_sentences):\n",
    "    chunks = sliding_window(s, win_size, step)\n",
    "    for chunk in chunks:\n",
    "        X_windowed.append(chunk)\n",
    "        Y_windowed.append(label_sentences[i])\n",
    "\n",
    "print(\"STEP 7: pad sliding windows\")\n",
    "padded_windows = pad_sequences(X_windowed, padding ='post')\n",
    "Y_windowed = np.array(Y_windowed)\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
